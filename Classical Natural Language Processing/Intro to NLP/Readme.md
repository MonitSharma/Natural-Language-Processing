### A code-first intro to Natural Language Processinng

All the lecture videos are available [here](insert link here)

This course was taught in the [University of San Francisco](https://) . The course is taught in python , using libraries like sklearn, nltk, pytorch, and fastai 

#### Table of contents
 The following will be covered by the codes:
 1. What is NLP?
 2. Topic Modeling with NMF and SVD
 3. Sentiment Classification with Naive Bayes, Logistic regression, and ngrams
 4. Regex ( and re-visitng tokenization)
 5. Language modeling & sentiment classification with deep learning
 6. Translation with RNNs
 7. Translation with the Transformer architecture
 8. Bias & Ethics in NLP

#### Comment on the order of content
    This course is strucutred with a top-down teaching methd, which is usually different from how most of the math courses are taught. In a bottom-up approach, we first learn all the separate components we'll be using and then gradually build them up in more complex structures. Harvard Professor David Perkins has a book, Making Learning Whole in which he uses baseball as an analogy. We don't require kids to memorize all the rules of baseball and understand all the technical details before we let them play the game. Rather, they start playing with a just general sense of it, and then gradually learn more rules/details as time goes on.

    Don't worry if you don't understand everything at once! You are not supposed to.

    To start, focus on what thing DO, not what things ARE.