{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What can you do with NLP?    \n",
    "     \n",
    "     NLP is a broad field, encompassing a variety of tasks, includinng:\n",
    "     1. part of speech tagging : identifying if each word is a noun, verb, adjective etc\n",
    "     2. Named enitiy recoginition : identify person names, organizations, locations, medical codes, time expressions, quantities, monetary values, etc\n",
    "\n",
    "     3. Question Answering\n",
    "     \n",
    "     4. Speech recoginition\n",
    "     5. Text-to-Speech and Speech-to-text\n",
    "     6. Topic modeling\n",
    "     7. Sentiment classifications\n",
    "     8. Language modeling\n",
    "     9. Translation\n",
    "\n",
    "    Many techniques from NLP are useful in a variety of places, for instance, you may have text within your tabular data.\n",
    "\n",
    "    There are also interesting techniques that let you go between texts and images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You'll learn\n",
    "     \n",
    "     1. Topic modeling\n",
    "     2. Sentiment classification\n",
    "     3. Language modeling\n",
    "     4. Translation\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Changing Field\n",
    "     Historically, NLP originally relied on hard-coded rules about a language. In the 1990s, there was a change towards using statistical & machine learning approaches, but the complexity of natural language meant that simple statistical approaches were often not state-of-the-art. Deep learning allows for much greater complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case Study \n",
    "##### Spell Checkers\n",
    "         \n",
    "         Historically, spell checkers required thoiusands of lines of hard-coded rules.\n",
    "\n",
    "         A version that uses historical data and probabilities can be written in far fewer lines.\n",
    "\n",
    "[Read more here](http://norvig.com/spell-correct.html)\n",
    "\n",
    "##### Norvig vs. Chomsky\n",
    "            This \"debate\" captures the tension between two approaches:\n",
    "            1. modeling the underlying mechanism of a phenomena\n",
    "            2. using ml to predict outputs(without necessarily undertsanding the mechansim that create them)\n",
    "\n",
    "            This tension is still very much present in NLP (and in many fields in which machine learning is being adopted, as well as in approachs to \"artificial intelligence\" in general).\n",
    "\n",
    "Background: Noam Chomsky is an MIT emeritus professor, the father of modern linguistics, one of the founders of cognitive science, has written >100 books. Peter Norvig is Director of Research at Google.\n",
    "\n",
    "    From MIT Tech Review coverage (2011):\n",
    "            \"Chomsky derided researchers in machine learning who use purely statistical methods to produce behavior that mimics something in the world, but who don’t try to understand the meaning of that behavior. Chomsky compared such researchers to scientists who might study the dance made by a bee returning to the hive, and who could produce a statistically based simulation of such a dance without attempting to understand why the bee behaved that way. “That’s a notion of scientific success that’s very novel. I don’t know of anything like it in the history of science,” said Chomsky.\"\n",
    "\n",
    "    From Norvig's response :\n",
    "            \"Breiman is inviting us to give up on the idea that we can uniquely model the true underlying form of nature's function from inputs to outputs. Instead he asks us to be satisfied with a function that accounts for the observed data well, and generalizes to new, previously unseen data well, but may be expressed in a complex mathematical form that may bear no relation to the \"true\" function's form (if such a true function even exists). Chomsky takes the opposite approach: he prefers to keep a simple, elegant model, and give up on the idea that the model will represent the data well.\"\n",
    "     \n",
    "[Norvig vs. Chomsky and the fight for the future of AI](https://www.tor.com/2011/06/21/norvig-vs-chomsky-and-the-fight-for-the-future-of-ai/)\n",
    "\n",
    "[An extended conversation](https://www.theatlantic.com/technology/archive/2012/11/noam-chomsky-on-where-artificial-intelligence-went-wrong/261637/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yann LeCun vs. Chris Manning\n",
    "    Another interesting discussion along the topic of how much linguistic structure to incorporate into NLP models is between Yann LeCun and Chris Manning:\n",
    "[Deep Learning, Structure and Innate Priors: A Discussion between Yann LeCun and Christopher Manning](http://www.abigailsee.com/2018/02/21/deep-learning-structure-and-innate-priors.html)\n",
    "\n",
    "    On one side, Manning is a prominent advocate for incorporating more linguistic structure into deep learning systems. On the other, LeCun is a leading proponent for the ability of simple but powerful neural architectures to perform sophisticated tasks without extensive task-specific feature engineering. For this reason, anticipation for disagreement between the two was high, with one Twitter commentator describing the event as “the AI equivalent of Batman vs Superman”.\n",
    "\n",
    "    Manning described structure as a “necessary good” (9:14), arguing that we should have a positive attitude towards structure as a good design decision. In particular, structure allows us to design systems that can learn more from less data, and at a higher level of abstraction, compared to those without structure.\n",
    "\n",
    "    Conversely, LeCun described structure as a “necessary evil” (2:44), and warned that imposing structure requires us to make certain assumptions, which are invariably wrong for at least some portion of the data, and may become obsolete within the near future. As an example, he hypothesized that ConvNets may be obsolete in 10 years (29:57).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resources:\n",
    "\n",
    "1. [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/), by Dan Jurafsky and James H. Martin (free PDF)\n",
    "2. [Introduction to Information Retrieval](https://nlp.stanford.edu/IR-book/html/htmledition/irbook.html) by By Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze (free online)\n",
    "3. [Natural Language Processing with PyTorch](https://learning.oreilly.com/library/view/natural-language-processing/9781491978221/) by Brian McMahan and Delip Rao (need to purchase or have O'Reilly Safari account)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP Tools\n",
    "    1. Regex\n",
    "    2. Tokenization\n",
    "    3. Word embeddings\n",
    "    4. Linear algebra/ matrix decomposition\n",
    "    5. Neural Networks\n",
    "    6. Hidden Markov Model\n",
    "    7. Parse Trees\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python Libraries \n",
    "1. nltk : very broad NLP library\n",
    "2. spaCy : create parse trees, excellinet tokenizer\n",
    "3. gensim : topic modeling and similarity detection\n",
    "4. PyText\n",
    "5. fastText : libarary of embeddings \n",
    "6. sklearn : general purpose Python ML library\n",
    "7. fastai : fast & accurate neural nets (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
